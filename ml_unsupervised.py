# -*- coding: utf-8 -*-
"""ML Unsupervised.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ErPIKWqPS6LMCXCcyCKDbY-hPjkARSA

# **Customer segmentation**
https://www.youtube.com/watch?v=JZSYll1j6QA

# Configurações
"""

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
import numpy as np

import scipy.cluster.hierarchy as sch #para dendograma
from sklearn.cluster import AgglomerativeClustering #para silhueta
from sklearn.metrics import silhouette_score #para silhueta

from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score,confusion_matrix

"""# Import data"""

df = pd.read_csv('Events Raw Data.csv')

df.shape

"""**Missing values**"""

df.isna().sum()

df[df['Total_Household_Income'].isnull()]

#Como as mesmas cinco observações tem todos os dados nulos, vamos remover:
df1 = df[df['Total_Household_Income'].isnull()==False].reset_index()
df1.isnull().sum()

"""#Features

**Análise descritiva**
"""

for columns in df1:
  unique_vals = pd.unique(df1[columns])
  count_unique_vals = len(unique_vals)

  if count_unique_vals < 10:
    print(columns,'(',count_unique_vals,')')
    print(unique_vals)
    print('\n')
  else:
    print(columns,'(',count_unique_vals,')')
    print('[over 10 unique values]')
    print('\n')

features_cat = ['Age', 'Gender', 'Current_Status', 'Total_Household_Income','How often you attend Entertaining events in a year?', 'Social_Media', 'How many hours are you willing to travel to attend an event?','Do you enjoy adrenaline-rush activities?','Are food areas, coffee areas, bars & toilets important to you?','What is your favourite attraction from below:','Were you satisfied with the last event you attended with us?','Would you recommend our events to other people?','Did you find our events value for money?']

for i in features_cat:
  order = sorted(df1[i].unique())
  sns.countplot(x=i, data= df1, order=order)
  plt.title(i)
  plt.xticks(rotation=90)
  plt.show()

"""**One Hot Encoding**"""

#Não usará a localização dos clientes (lat, long, district, postcode,Constituency)

#K-means só aceita variáveis contínuas
df2 = pd.get_dummies(df1[features_cat], columns = features_cat).astype(int)

"""**Scale**"""

# Inicializar o scaler de normalização (dados normais, com média 0 e desvio padrão 1)
scaler = StandardScaler()

# Aplicar a padronização (Z-Score)
df_standardized = pd.DataFrame(scaler.fit_transform(df2), columns=df2.columns)

# Inicializar o scaler de normalização (dados entre 0 e 1)
scaler = MinMaxScaler()

# Aplicar a padronização
df_normalized = pd.DataFrame(scaler.fit_transform(df2), columns=df2.columns)

#Como aqui todos os dados são flag, não faz diferença

"""#Componentes principais"""

#Gera componentes principais
n_components = 60

pca = PCA(n_components=n_components, random_state = 22)
PCA_evaluate = pca.fit(df_normalized).transform(df_normalized)

print(df_normalized.shape)
print(PCA_evaluate.shape)

explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = explained_variance_ratio.cumsum()

# Creating a df with the components qt and cumulative_variance_ratio
df_cumulative_variance_ratio = zip(range(1,n_components+1), cumulative_variance_ratio)
df_cumulative_variance_ratio = pd.DataFrame(df_cumulative_variance_ratio, columns=["PCA Comp", "Cum Explained Variance"])

#Calcula quantidade de componentes necessárias para explicar 95% da variância
n_components_95 = df_cumulative_variance_ratio[df_cumulative_variance_ratio["Cum Explained Variance"] >= 0.95]["PCA Comp"].iloc[0]
print('Quantidade de componentes para atingir 95% de variância:', n_components_95)

# Plot the cumulative explained variance
plt.figure(figsize=(15, 5))
plt.plot(range(1, n_components + 1), cumulative_variance_ratio, marker='o', linewidth=2, c="r")
plt.title('Variancia explicada acumulada')
plt.ylabel('Variancia explicada acumulada')
plt.xlabel('# componentes principais')
plt.xticks(range(1, n_components + 1))

# adding arrow no acumulado 95%
plt.annotate('95% variance', xy=(n_components_95, cumulative_variance_ratio[n_components_95]),
            arrowprops=dict(facecolor='black', shrink=0.05))
plt.show()

#Gera base de treinamento com a quantidade de PCAs necessários para bater 95% da variância total
pca_train = PCA(n_components=n_components_95, random_state = 22)
X_train = pca_train.fit(df_normalized).transform(df_normalized)

"""#Método hierárquico (definir k)

**Dendograma**
"""

# Criar o dendrograma
plt.figure(figsize=(7, 5))
dendrogram = sch.dendrogram(sch.linkage(X_train, method='ward'))

# Exibir o dendrograma
plt.title('Dendrograma')
plt.xlabel('Amostras')
plt.ylabel('Distância Euclidiana')
plt.show()

"""**Cotovelo**"""

# Lista para armazenar as somas dos erros quadrados (inertia)
sse = []

# Testar diferentes números de clusters
range_n_clusters = range(1, 15)
for n_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=0)
    kmeans.fit(X_train)
    sse.append(kmeans.inertia_)  # A inércia é a soma dos erros quadrados

# Plotar o gráfico do Método do Cotovelo
plt.figure(figsize=(7, 5))
plt.plot(range_n_clusters, sse, 'bo-', label="SSE (Inertia)")
plt.xlabel("Número de Clusters")
plt.ylabel("SSE (Sum of Squared Errors)")
plt.title("Método do Cotovelo")
plt.grid(True)
plt.show()

"""**Silhueta**"""

range_n_clusters = list(range(2, 15)) # Testar diferentes números de clusters neste intervalo
silhouette_avg = []

for n_clusters in range_n_clusters:
    # Aplicar o clustering hierárquico
    clusterer = AgglomerativeClustering(n_clusters=n_clusters)
    cluster_labels = clusterer.fit_predict(X_train)

    # Calcular o coeficiente de silhueta
    silhouette_avg.append(silhouette_score(X_train, cluster_labels))

# Melhor número de clusters (silhueta máxima)
melhor_n_clusters = range_n_clusters[np.argmax(silhouette_avg)]

# Exibir os resultados
plt.figure(figsize=(7, 5))
plt.plot(range_n_clusters, silhouette_avg, 'bo-', label="Silhueta média")
plt.xlabel("Número de Clusters")
plt.ylabel("Coeficiente de Silhueta")
plt.suptitle("Silhueta para diferentes números de Clusters")
plt.title(f'Quantidade para silhueta máxima: {melhor_n_clusters}', fontsize=7)
plt.legend()
plt.show()

"""#Método não hierárquico (model training)"""

#Pelos métodos hierárquicos, o ideal seria entre 6 e 8 clusters

k=6

kmeans = KMeans(n_clusters=k, random_state=22)
kmeans = kmeans.fit(X_train)

# Prints the Inertia
print("The Inertia is: ", kmeans.inertia_)

print(X_train.shape[0])
print(kmeans.labels_.shape) #marcação de um cluster por observação

"""#Valida"""

#Cria base com output e nomes dos cluster

cluster_output = pd.DataFrame(kmeans.labels_)
cluster_output = cluster_output.rename(columns={0: 'cluster_num'})

#nomes_cluster = ['1.legal','2.bonito','3.bacana','4.diferente','5.exotico','6.inteligente']

def nome_cluster(row):
    for i in range(k):
      if row['cluster_num'] == i:
        return 'Cluster '+str(i+1)  #se quiser de chamar de Cluster N
        #return nomes_cluster[i]   # se tiver nome definidor
    else:
        return 'unknown cluster'

cluster_output['cluster_nome'] = cluster_output.apply(nome_cluster, axis=1)

pd.crosstab(cluster_output['cluster_num'],cluster_output['cluster_nome'])

"""**Volumetria**"""

volume_cluster = cluster_output.groupby('cluster_nome').count()

#Gráfico de barras
plt.figure(figsize=(3, 2))
ax = volume_cluster.plot(kind='bar', legend=False)

# Adicionando rótulos de dados
for p in ax.patches:
    ax.annotate(str(p.get_height()),
                (p.get_x() + p.get_width() / 2.,
                 p.get_height()),
                 ha='center', va='top', xytext=(0, 10), textcoords='offset points')

# Forçando o eixo Y a ir até 5000 - para o rótulo de dados caber
ax.set_ylim(0, 5000)

# Definindo os rótulos dos eixos e o título
plt.xlabel('Cluster')
plt.ylabel('Counts')
plt.title('Volumetria por cluster')

# Exibindo o gráfico
plt.show()

"""**Plot da separação**"""

# Plotting the data
plt.figure()
plt.figure(figsize=(8,4))

colors = ['navy', 'turquoise', 'darkorange', 'red', 'gray', 'green', 'pink', 'purple']
#colors = list(matplotlib.colors.cnames.keys())[:k]

for color, i, cluster_name in zip(colors, range(k), cluster_names):
    #plot das observações pelas duas primeiras componentes - um plot por cluster
    plt.scatter(X_train[cluster_output['cluster_num'] == i, 0], #primeira componente principal
                X_train[cluster_output['cluster_num'] == i, 1], #segunda componente principal
                color=color,
                label=nomes_cluster[i])

plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.6)
plt.title('Observações pelas duas primeiras componentes')
plt.show()

"""**Validação por árvore**"""

# Definir o modelo de árvore de decisão com máximo de k nós finais
tree_model = DecisionTreeClassifier(max_leaf_nodes=k, random_state=42)

tree_model.fit(X_train, cluster_output['cluster_num'])
y_pred = tree_model.predict(X_train)

#compara o cluster origem com a saída da arvore
accuracy = accuracy_score(cluster_output['cluster_num'], y_pred)

cm = confusion_matrix(cluster_output['cluster_num'], y_pred, normalize='true')

plt.figure(figsize=(4,4))

cm_plot = sns.heatmap(cm, annot=True, fmt=".2f", cmap="Blues")
cm_plot.xaxis.set_ticks_position('top')
cm_plot.xaxis.set_label_position('top')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.suptitle(f"Acurácia: {accuracy:.2f}",fontweight='bold')
plt.subplots_adjust(top=0.7)
plt.show()

cm = confusion_matrix(cluster_output['cluster_num'], y_pred, normalize='pred')

plt.figure(figsize=(4,4))
cm_plot = sns.heatmap(cm, annot=True, fmt=".2f", cmap="Blues")
cm_plot.xaxis.set_ticks_position('top')
cm_plot.xaxis.set_label_position('top')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.suptitle(f"Acurácia: {accuracy:.2f}",fontweight='bold')
plt.subplots_adjust(top=0.7)
plt.show()

"""#Interpretação"""

#Gera base com as variáveis originais e os clusters finais
df_results = pd.concat([df1,cluster_output],axis=1)
df_results = df_results.sort_values('cluster_nome')

df_results['Age_n1'] = df_results['Age'].str[:2].astype(int)
df_results['Age_n2'] = df_results['Age'].str[3:5]
df_results['Age_n2'] = df_results['Age_n2'].replace('or','80')
df_results['Age_n2'] = df_results['Age_n2'].astype(int)
df_results['Age_pontomedio'] = (df_results['Age_n1'] + df_results['Age_n2'])/2

plt.figure(figsize=(6, 3))
sns.boxplot(x='cluster_nome', y='Age_pontomedio', data=df_results)

# Adicionando rótulos e título
plt.xlabel('Cluster')
plt.ylabel('Valor')
plt.title('Boxplot de Idade por Cluster')

# Exibindo o gráfico
plt.show()

percent_gender = pd.crosstab(df_results['cluster_nome'], df_results['Gender'],normalize='index')

plt.figure(figsize=(6,2))
percent_gender.plot(kind='bar', stacked=True)
plt.title('Proporção de gênero por Cluster (100%)')
plt.xlabel('Cluster')
plt.ylabel('Proporção')
plt.show()

df_results['events_year_n'] = df_results['How often you attend Entertaining events in a year?'].str[:1].astype(int)

plt.figure(figsize=(6, 3))
sns.boxplot(x='cluster_nome', y='events_year_n', data=df_results)

# Adicionando rótulos e título
plt.xlabel('Cluster')
plt.ylabel('Valor')
plt.title('Boxplot de qtd. eventos por Cluster')

# Exibindo o gráfico
plt.show()

"""#Escorar nova base"""

#garantir que nova_base tenha as mesmas variáveis com mesmos tratamentos de X_train
#clusters_nova_base = kmeans.predict(nova_base)